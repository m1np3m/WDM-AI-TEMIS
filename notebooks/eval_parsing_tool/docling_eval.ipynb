{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "eval_data = json.load(open(\"C:\\\\Users\\\\PC\\\\CODE\\\\WDM-AI-TEMIS\\\\data-finetune\\\\final_data\\\\docling_json.json\", \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6\n",
    "\n",
    "ground_truth = eval_data[idx][\"markdown_content\"]\n",
    "predict = eval_data[idx][\"docling_md\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|\\n|--|--|--|--|--|--|--|--|--|--|--|--|--|--|\\n|Month| Jan| Feb| Mar| Apr| May| Jun| Jul| Aug| Sep| Oct| Nov| Dec| Year|\\n|Mean dailymaximum °C (°F)|26(79)|27(81)|25(77)|23(73)|20(68)|17(63)|17(63)|20(68)|23(73)|27(81)|27(81)|28(82)|23(74)|\\n|Mean dailyminimum °C (°F)|15(59)|15(59)|13(55)|6(43)|6(43)|3(37)|1(34)|0(32)|5(41)|12(54)|14(57)|16(61)|9(48)|\\n|Average rainfallmm (inches)|76(3.0)|98(3.9)|55(2.2)|28(1.1)|50(2.0)|34(1.3)|0(0)|0(0)|0(0)|22(0.9)|103(4.1)|68(2.7)|534(21.0)|\\n|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|\\n|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'| Climate data for Berea District   | Climate data for Berea District   | Climate data for Berea District   | Climate data for Berea District   | Climate data for Berea District   | Climate data for Berea District   | Climate data for Berea District   | Climate data for Berea District   | Climate data for Berea District   | Climate data for Berea District   | Climate data for Berea District   | Climate data for Berea District   | Climate data for Berea District   | [hide]        |\\n|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|---------------|\\n| Month                             | Jan                               | Feb                               | Mar                               | Apr                               | May                               | Jun                               | Jul                               | Aug                               | Sep                               | Oct                               | Nov                               | Dec                               | Year          |\\n| Mean daily                        | 26 (79)                           | 27 (81)                           | (77)                              | 23 (73)                           | 20 (68)                           | 17 (63)                           | 17 (63)                           | 20 (68)                           | 23 (73)                           | 27 (81)                           | 27 (81)                           | 28 (82)                           | 23 (74)       |\\n| Mean daily                        | 15 (59)                           | 15 (59)                           | 13 (55)                           | 6 (43)                            | 6 (43)                            | 3                                 | 1 (34)                            | 0 (32)                            | 5 (41)                            | 12 (54)                           | 14 (57)                           | 16 (61)                           | 9 (48)        |\\n| Average rainfall mm (inches)      | 76 (3.0)                          | 98 (3.9)                          | 55 (2.2)                          | 28 (1.1)                          | 50 (2.0)                          | 34 (1.3)                          | (0)                               | 0 (0)                             | 0 (0)                             | 22 (0.9)                          | 103 (4.1)                         | 68 (2.7)                          | 534 (21.0)    |\\n| Source 1: [5]                     | Source 1: [5]                     | Source 1: [5]                     | Source 1: [5]                     | Source 1: [5]                     | Source 1: [5]                     | Source 1: [5]                     | Source 1: [5]                     | Source 1: [5]                     | Source 1: [5]                     | Source 1: [5]                     | Source 1: [5]                     | Source 1: [5]                     | Source 1: [5] |\\n| Source 2: [6]                     | Source 2: [6]                     | Source 2: [6]                     | Source 2: [6]                     | Source 2: [6]                     | Source 2: [6]                     | Source 2: [6]                     | Source 2: [6]                     | Source 2: [6]                     | Source 2: [6]                     | Source 2: [6]                     | Source 2: [6]                     | Source 2: [6]                     | Source 2: [6] |'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: LLM initialized with JSON mode.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"Lỗi: OPENAI_API_KEY không được tìm thấy.\")\n",
    "else:\n",
    "    llm = ChatOpenAI(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        model_kwargs={\n",
    "            \"response_format\": {\"type\": \"json_object\"}\n",
    "        }\n",
    "    )\n",
    "    print(\"DEBUG: LLM initialized with JSON mode.\")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "def evaluate_table_extraction(ground_truth: str, predict: str, llm_model = None, debug: bool = False) -> any: # Thay đổi kiểu trả về nếu cần\n",
    "   parser = JsonOutputParser()\n",
    "\n",
    "   prompt_template_str = \"\"\"\n",
    "   You are provided with two markdown tables: a ground truth table and a predicted table.\n",
    "\n",
    "Ground Truth Table:\n",
    "\n",
    "{ground_truth}\n",
    "\n",
    "Predicted Table:\n",
    "\n",
    "{predict}\n",
    "\n",
    "Evaluate the similarity between the two tables on a scale of 0 to 1, where 0 indicates no similarity and 1 indicates perfect match. For table fragments (tables without headers), prioritize the content and structure of the tables over the header values.\n",
    "\n",
    "Provide a similarity score and a brief explanation of your reasoning.\n",
    "You must respond with a JSON object containing only a score field with a float value between 0 and 1.\n",
    "Remember: Your entire response must be a valid JSON object with only a \"score\" field containing a number between 0 and 1.\n",
    "   \"\"\"\n",
    "   prompt = PromptTemplate.from_template(template=prompt_template_str)\n",
    "\n",
    "   try:\n",
    "      chain = prompt | llm_model | parser\n",
    "      if debug:\n",
    "         print(\"DEBUG: Chain created successfully.\")\n",
    "\n",
    "      invocation_payload = {\n",
    "         \"ground_truth\": ground_truth,\n",
    "         \"predict\": predict\n",
    "      }\n",
    "      if debug:\n",
    "         print(f\"DEBUG: Invoking chain with payload (first 100 chars of each): \\nGround truth: {ground_truth[:100]}...\\nPredict: {predict[:100]}...\")\n",
    "\n",
    "      # Để kiểm tra riêng lẻ LLM (bỏ qua parser tạm thời):\n",
    "      # formatted_prompt = prompt.invoke(invocation_payload)\n",
    "      # print(f\"DEBUG: Formatted prompt sent to LLM:\\n{formatted_prompt.to_string()}\") # Hoặc .text tùy phiên bản\n",
    "      # llm_output = llm_model.invoke(formatted_prompt)\n",
    "      # print(f\"DEBUG: Raw output from LLM:\\n{llm_output}\")\n",
    "      # res = parser.parse(llm_output) # Parse thủ công nếu muốn kiểm tra parser\n",
    "\n",
    "      res = chain.invoke(invocation_payload)\n",
    "      if debug:\n",
    "         print(f\"DEBUG: Chain invocation successful. Result (res): {res}\")\n",
    "         print(f\"DEBUG: Type of res: {type(res)}\")\n",
    "      return res\n",
    "\n",
    "   except Exception as e:\n",
    "      if debug:   \n",
    "         print(f\"ERROR in evaluate_table_extraction: {e}\")\n",
    "         import traceback\n",
    "         traceback.print_exc() # In ra chi tiết lỗi và dòng gây lỗi\n",
    "      return \"\" # Hoặc None, hoặc một giá trị báo lỗi cụ thể"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Chain created successfully.\n",
      "DEBUG: Invoking chain with payload (first 100 chars of each): \n",
      "Ground truth: |Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea ...\n",
      "Predict: | Climate data for Berea District   | Climate data for Berea District   | Climate data for Berea Dis...\n",
      "DEBUG: Chain invocation successful. Result (res): {'score': 0.85}\n",
      "DEBUG: Type of res: <class 'dict'>\n",
      "{'score': 0.85}\n"
     ]
    }
   ],
   "source": [
    "a = evaluate_table_extraction(ground_truth, predict, llm, debug=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/163 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [02:46<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7034969325153373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "total_score = 0\n",
    "\n",
    "\n",
    "for sample in tqdm(eval_data):\n",
    "    ground_truth = sample['markdown_content']\n",
    "    predict = sample['docling_md']\n",
    "    score = evaluate_table_extraction(ground_truth, predict, llm)\n",
    "    total_score += score['score']\n",
    "    # print(score)\n",
    "print(total_score / len(eval_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 16/163 [00:47<07:20,  3.00s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from gemini import *\n",
    "from gemini import generate\n",
    "import time\n",
    "\n",
    "total_score = 0\n",
    "max_retries = 3  # Maximum number of retries for handling connection errors\n",
    "\n",
    "for sample in tqdm(eval_data):\n",
    "    ground_truth = sample['markdown_content']\n",
    "    predict = sample['docling_md']\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            score = generate(ground_truth, predict)\n",
    "            total_score += score\n",
    "            break  # Exit the retry loop if successful\n",
    "        except ConnectionResetError as e:\n",
    "            print(f\"Connection error: {e}. Retrying {retries + 1}/{max_retries}...\")\n",
    "            retries += 1\n",
    "            time.sleep(2)  # Wait for 2 seconds before retrying\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            break  # Exit the retry loop on unexpected errors\n",
    "\n",
    "if len(eval_data) > 0:\n",
    "    print(total_score / len(eval_data))\n",
    "else:\n",
    "    print(\"No data to evaluate.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
