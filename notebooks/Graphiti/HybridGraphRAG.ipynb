{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d1ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from graphiti.graph import KnowledgeGraph\n",
    "from neo4j_graphrag.retrievers import QdrantNeo4jRetriever\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11045009",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "qdrant_key = os.getenv(\"QDRANT_KEY\")\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "neo4j_uri = os.getenv(\"NEO4J_URI\")\n",
    "neo4j_username = os.getenv(\"NEO4J_USERNAME\")\n",
    "neo4j_password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c9d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_username, neo4j_password))\n",
    "\n",
    "collection_name = \"graphRAGstoreds\"\n",
    "qdrant_client = QdrantClient(url=qdrant_url, api_key=qdrant_key)\n",
    "\n",
    "try:\n",
    "    qdrant_client.delete_collection(collection_name=collection_name)\n",
    "    print(f\"Collection '{collection_name}' has been deleted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not delete collection (it might not exist, which is OK): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b62e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=openai_key,\n",
    ")\n",
    "\n",
    "def openai_llm_parser(prompt_input: str) -> KnowledgeGraph:\n",
    "    \"\"\"\n",
    "    Extracts structured information using a generalized prompt, \n",
    "    allowing the LLM to infer entity types and relationships.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    system_prompt = f\"\"\"Your task is to act as an expert information extractor. From the provided INPUT_TEXT, you will extract a knowledge graph.\n",
    "\n",
    "    The output must be a JSON object with a single key \"graph\", which contains a list of structured objects. Each object represents a relationship triplet and must have the following keys: 'h', 'type_h', 'r', 'o', 'type_t'.\n",
    "\n",
    "    GUIDELINES:\n",
    "    1.  'h' (head) and 'o' (tail) are the entities.\n",
    "    2.  'type_h' and 'type_t' are the general categories. You must infer these types. Types should be concise, capitalized, singular nouns (e.g., PERSON, COMPANY, VEHICLE, LOCATION, PRODUCT).\n",
    "    3.  **Crucially, identify abstract concepts like EVENTS (e.g., 'Battle of New York', 'Ultron's Attack') and PROTOCOLS (e.g., 'Sokovia Accords').**\n",
    "    4.  'r' (relationship) is a short, active verb.\n",
    "      - For actions between entities, use verbs like: Drove, Invented, Created, Wields, Led, Defeated.\n",
    "      - **For cause-and-effect, use verbs like: Caused, LedTo, ResultedIn.**\n",
    "      - **For participation, use: ParticipatedIn.**\n",
    "    5.  **Entity Disambiguation**: Consolidate different names for the same entity.\n",
    "    6.  **Simplicity**: Keep entity names short and specific.\n",
    "\n",
    "    EXAMPLE 1 (Business):\n",
    "    - Input: 'The 2008 financial crisis led to the creation of the Dodd-Frank Act.'\n",
    "    - Output:\n",
    "    {{\n",
    "      \"graph\": [\n",
    "        {{ \"h\": \"2008 Financial Crisis\", \"type_h\": \"EVENT\", \"r\": \"LedTo\", \"o\": \"Dodd-Frank Act\", \"type_t\": \"PROTOCOL\" }}\n",
    "      ]\n",
    "    }}\n",
    "\n",
    "    EXAMPLE 2 (MCU - a more relevant example for you):\n",
    "    - Input: 'The Battle of New York was a major conflict where the Avengers first assembled to fight Loki.'\n",
    "    - Output:\n",
    "    {{\n",
    "      \"graph\": [\n",
    "          {{ \"h\": \"Avengers\", \"type_h\": \"GROUP\", \"r\": \"ParticipatedIn\", \"o\": \"Battle of New York\", \"type_t\": \"EVENT\" }},\n",
    "          {{ \"h\": \"Loki\", \"type_h\": \"PERSON\", \"r\": \"ParticipatedIn\", \"o\": \"Battle of New York\", \"type_t\": \"EVENT\" }}\n",
    "      ]\n",
    "    }}\n",
    "\n",
    "    Your output MUST be a valid JSON object. Do not add any text before or after the JSON.\n",
    "\n",
    "    ===========================================================\n",
    "    INPUT_TEXT:\n",
    "    {prompt_input}\n",
    "    \"\"\"\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen/qwen3-235b-a22b:free\",\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    response_content = completion.choices[0].message.content\n",
    "    print(\"Raw response from model:\", response_content)\n",
    "\n",
    "    try:\n",
    "        return KnowledgeGraph.model_validate_json(response_content)\n",
    "    except Exception as e:\n",
    "        print(f\"Pydantic validation failed: {e}\")\n",
    "        return KnowledgeGraph(graph=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRAGPipeline:\n",
    "    def __init__(self, pdf_folder: str, qdrant_url: str, graph_name: str):\n",
    "        self.pdf_folder = pdf_folder\n",
    "        self.qdrant_url = qdrant_url\n",
    "        self.graph_name = graph_name\n",
    "\n",
    "        self.embedding_model = OpenAIEmbeddings()\n",
    "        self.llm = ChatOpenAI(temperature=0)\n",
    "        self.graph = KnowledgeGraph(name=graph_name, db=\"mongo\")\n",
    "        self.qdrant_client = QdrantClient(qdrant_url)\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "\n",
    "        self.documents = []\n",
    "        self.chunks = []\n",
    "\n",
    "    def load_pdfs(self):\n",
    "        all_pdfs = list(Path(self.pdf_folder).glob(\"*.pdf\"))\n",
    "        for file in all_pdfs:\n",
    "            loader = PyPDFLoader(str(file))\n",
    "            docs = loader.load()\n",
    "            self.documents.extend(docs)\n",
    "        print(f\"‚úÖ Loaded {len(self.documents)} documents from {len(all_pdfs)} PDFs.\")\n",
    "\n",
    "    def split_documents(self):\n",
    "        self.chunks = self.text_splitter.split_documents(self.documents)\n",
    "        for i, chunk in enumerate(self.chunks):\n",
    "            chunk.metadata[\"id\"] = f\"chunk_{i}\"\n",
    "        print(f\"üß© Split into {len(self.chunks)} chunks.\")\n",
    "\n",
    "    def index_embeddings_to_qdrant(self):\n",
    "        vectorstore = Qdrant.from_documents(\n",
    "            documents=self.chunks,\n",
    "            embedding=self.embedding_model,\n",
    "            qdrant_client=self.qdrant_client,\n",
    "            collection_name=self.graph_name,\n",
    "            ids=[chunk.metadata[\"id\"] for chunk in self.chunks]\n",
    "        )\n",
    "        print(\"üì¶ Stored vectors into Qdrant.\")\n",
    "\n",
    "    def create_ontology_graph(self):\n",
    "        for chunk in self.chunks:\n",
    "            content = chunk.page_content\n",
    "            ontology_prompt = f\"Extract key ontology concepts from the text:\\n\\n{content}\"\n",
    "            ontology = self.llm.predict(ontology_prompt)\n",
    "            self.graph.add_node(content, metadata={\"id\": chunk.metadata[\"id\"], \"ontology\": ontology})\n",
    "\n",
    "        self.graph.link_similar_nodes()\n",
    "        self.graph.save()\n",
    "        print(\"üåê Created semantic graph in Graphiti.\")\n",
    "        \n",
    "    def hybrid_retrieve_and_answer(self, query: str, top_k: int = 3):\n",
    "        # Semantic retrieval\n",
    "        vectorstore = Qdrant(\n",
    "            client=self.qdrant_client,\n",
    "            collection_name=self.graph_name,\n",
    "            embeddings=self.embedding_model\n",
    "        )\n",
    "        semantic_docs = vectorstore.similarity_search(query, k=top_k)\n",
    "\n",
    "        # Semantic graph retrieval\n",
    "        graph_docs = self.graph.find_related(query, top_k=top_k)\n",
    "\n",
    "        # K·∫øt h·ª£p\n",
    "        context = \"\\n\\n\".join(\n",
    "            [doc.page_content for doc in semantic_docs] +\n",
    "            [node[\"content\"] for node in graph_docs]\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\"Answer the question based on the following context:\\n\\n{context}\\n\\nQuestion: {query}\"\"\"\n",
    "        answer = self.llm.predict(prompt)\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4dc22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = HybridRAGPipeline(\n",
    "    pdf_folder=\"pdf_docs\",                # üóÇÔ∏è th∆∞ m·ª•c ch·ª©a file .pdf\n",
    "    qdrant_url=\"http://localhost:6333\",   # üåê URL Qdrant local\n",
    "    graph_name=\"carbon_hybrid_rag\"        # üß† T√™n collection/graph\n",
    ")\n",
    "\n",
    "pipeline.load_pdfs()\n",
    "pipeline.split_documents()\n",
    "pipeline.index_embeddings_to_qdrant()\n",
    "pipeline.create_ontology_graph()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
