# -*- coding: utf-8 -*-
"""Evaluator_fullpipeline_main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R13SpwtuPWcl46Xjn2jPGSidxlu55FjE

## Khởi tạo
"""

import os
import sys

__script_path = os.path.abspath(globals().get("__file__", "."))
__script_dir = os.path.dirname(__script_path)
root_dir = os.path.abspath(f"{__script_dir}/../..")
print(root_dir)
for lib in [root_dir][::-1]:
    if lib in sys.path:
        sys.path.remove(lib)
    sys.path.insert(0, lib)

from configs.config import *
from libs.common import *
from utils.format_utils import *

# from utils.extract_tables import full_pipeline
from utils.rag_evaluation import *
from utils.rag_qdrant_utils import *

load_dotenv(find_dotenv())

os.makedirs(exps_dir, exist_ok=True)

mongo_uri = os.getenv("MONGO_URI")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
QUOTIENTAI = os.getenv("QuotientAI")
collection = "kyanon_digital"

# mongo_client = get_mongo_client(mongo_uri)
# db = mongo_client['Kyanon']
# collection = db['RAG']
collection_name = "kyanon_digital"
QUOTIENT_API_KEY = os.getenv("QUOTIENT_API_KEY")


import torch

print(torch.cuda.is_available())

##Uncomment to initialise qdrant client in memory
# client.close()
client = qdrant_client.QdrantClient(
    path=f"{exps_dir}/qdrant_client_memory",
)

# ##Uncomment below to connect to Qdrant Cloud
# client = qdrant_client.QdrantClient(
#     os.environ.get("QDRANT_URL"),
#     api_key=os.environ.get("QDRANT_API_KEY"),
# )

## Uncomment below to connect to local Qdrant
# client = qdrant_client.QdrantClient("http://localhost:6333")

# from fastembed.embedding import TextEmbedding

# data=pd.DataFrame(TextEmbedding.list_supported_models())
# data

# data.model.unique()

# # client.create_collection(
# #     collection_name='kyanon_digital',
# #     vectors_config=VectorParams(size=768, distance=Distance.COSINE),
# # )
# client.get_collections()

# info = client.get_collection(collection_name="experiment_512_128_gte-base")
# print(info)

# import requests
# import json

# def summary_fn(prompt):
#     """
#     Gửi prompt đến Ollama và nhận về một đoạn phản hồi hoàn chỉnh.

#     Tham số:
#         prompt (str): Câu hỏi hoặc yêu cầu bạn muốn gửi đến mô hình.

#     Trả về:
#         str: Phản hồi hoàn chỉnh từ mô hình.
#     """
#     response = requests.post(
#   url="https://openrouter.ai/api/v1/chat/completions",
#   headers={
#     "Authorization": f"Bearer {os.getenv('API_OPENROUTE')}",
#     "Content-Type": "application/json",
#   },
#   data=json.dumps({
#     "model": "mistralai/devstral-small:free",
#     "messages": [
#       {
#         "role": "user",
#         "content": [
#           {
#             "type": "text",
#             "text": f"""
#     You are given a single table fragment extracted from a PDF document using OCR. This fragment may be part of a larger table that was split across multiple pages due to formatting or page layout.
#     Your task is to generate a detailed and comprehensive summary of the content in this table fragment.
#     The summary should clearly describe:
#         The main subject or topic of the table
#         Key columns and their meanings
#         Important patterns, trends, or observations in the data
#         Any notable values or anomalies
#         Contextual information needed to understand the data
#     Write the summary in 1–3 full sentences , using clear and precise language.
#     If applicable, mention that this fragment appears to be part of a larger table, and include any inferred continuity from the data.
#     {prompt}
# """
#           }
#         ]
#       }
#     ],

#   })
# )

#     return response.json().get("choices", [{}])[0].get("message", {}).get("content", "")

# def extract_all_table(folder_path, output_json_filename="./final_tables.json"):
#     all_results = {}

#     for filename in os.listdir(folder_path):
#         if filename.lower().endswith(".pdf"):
#             pdf_path = os.path.join(folder_path, filename)
#             json_result = full_pipeline(source_path=pdf_path, verbose=1)

#             for key, value_list in json_result.items():
#                 if key not in all_results:
#                     all_results[key] = []
#                 all_results[key].extend(value_list)

#     try:
#         output_path = os.path.join(folder_path, output_json_filename)
#         with open(output_path, 'w', encoding='utf-8') as f:
#             json.dump(all_results, f, ensure_ascii=False, indent=4)
#         print(f"✅ Đã lưu toàn bộ kết quả vào: {output_json_filename}")
#     except Exception as e:
#         print(f"[LỖI] Không thể ghi file JSON tổng hợp: {e}")

#     return all_results

# table_results = extract_all_table(folder_path=f"{data_dir}/pdf/")

# table_results

from collections import defaultdict

from langchain.docstore.document import Document as LangchainDocument

qa_path = f"{data_dir}/QA_tables/fixed_label_QA.json"


def extract_unique_tables_from_qa():
    """
    Trích xuất các table unique từ file fixed_.json và tạo LangChainDocument
    """
    # Đọc file QA
    with open(qa_path, "r", encoding="utf-8") as f:
        qa_data = json.load(f)

    # Dictionary để lưu trữ table unique
    # Key: (source, table_idx)
    # Value: table information
    unique_tables = {}

    # Duyệt qua tất cả các câu hỏi để tìm table unique
    for qa_item in qa_data:
        source = qa_item["source"]
        table_idx = qa_item["table_idx"]
        page_numbers = qa_item["page_numbers"]
        context = qa_item["context"]

        # Tạo key unique cho table
        table_key = (source, table_idx)

        # Nếu chưa có table này trong dictionary, thêm vào
        if table_key not in unique_tables:
            unique_tables[table_key] = {
                "source": source,
                "table_idx": table_idx,
                "page_numbers": page_numbers,
                "table_content": context,
            }
        else:
            # Cập nhật page_numbers nếu có thêm trang mới
            existing_pages = set(unique_tables[table_key]["page_numbers"])
            new_pages = set(page_numbers)
            all_pages = sorted(list(existing_pages.union(new_pages)))
            unique_tables[table_key]["page_numbers"] = all_pages

    # Tạo danh sách LangChainDocument
    table_documents = []

    for table_key, table_info in unique_tables.items():
        table_document = LangchainDocument(
            page_content=table_info["table_content"],
            metadata={
                "source": table_info["source"],
                "page_numbers": table_info["page_numbers"],
                "is_table": True,
                "source_table_idx": table_info["table_idx"],
            },
        )
        table_documents.append(table_document)

    return table_documents


def get_tables_statistics():
    """
    Thống kê về các table unique được tìm thấy
    """
    with open(qa_path, "r", encoding="utf-8") as f:
        qa_data = json.load(f)

    # Thống kê theo source
    source_stats = defaultdict(set)
    total_qa_pairs = len(qa_data)

    for qa_item in qa_data:
        source = qa_item["source"]
        table_idx = qa_item["table_idx"]
        source_stats[source].add(table_idx)

    print(f"Tổng số câu hỏi-trả lời: {total_qa_pairs}")
    print(f"Số file/source unique: {len(source_stats)}")

    total_unique_tables = sum(
        len(table_indices) for table_indices in source_stats.values()
    )
    print(f"Tổng số table unique: {total_unique_tables}")

    print("\nThống kê theo source:")
    for source, table_indices in source_stats.items():
        print(
            f"  {source}: {len(table_indices)} table(s) - table_idx: {sorted(list(table_indices))}"
        )

    return source_stats


stats = get_tables_statistics()

import pymupdf


def get_detail_chunks(pdf_path):
    doc = pymupdf.open(pdf_path)
    source = os.path.splitext(os.path.basename(pdf_path))[0]

    page_documents = []
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        text = page.get_text().strip()
        if not text:
            continue

        page_document = LangchainDocument(
            page_content=text,
            metadata={
                "source": source,
                "page_numbers": [page_num + 1],
                "is_table": False,
            },
        )
        page_documents.append(page_document)

    doc.close()
    return page_documents


def process_all_pdfs_in_folder(folder_path):
    all_page_documents = []
    all_table_documents = []

    table_docs = extract_unique_tables_from_qa()
    all_table_documents.extend(table_docs)

    for filename in os.listdir(folder_path):
        if filename.lower().endswith(".pdf"):
            pdf_path = os.path.join(folder_path, filename)
            print(f"\n>>> Đang xử lý: {pdf_path}")
            page_docs = get_detail_chunks(pdf_path)
            all_page_documents.extend(page_docs)

    return all_page_documents, all_table_documents


print(data_dir)

all_page_documents, all_table_documents = process_all_pdfs_in_folder(
    f"{data_dir}/QA_tables/pdf/"
)

print(len(all_page_documents))
print(len(all_table_documents))

"""## Evaluation"""

import json

with open(f"{data_dir}/QA_tables/fixed_label_QA.json", "r", encoding="utf-8") as f:
    eval_data = json.load(f)

eval_df = pd.DataFrame(eval_data)
eval_df.head()

len(eval_df)

from langchain_google_vertexai import ChatVertexAI
from langchain_openai import ChatOpenAI
from ragas.embeddings import LangchainEmbeddingsWrapper

# from google.oauth2 import service_account
# credentials_path = "C:/Users/PC/Data/WDM-AI-TEMIS/gdsc2025-74596a254ab4.json"
# credentials = service_account.Credentials.from_service_account_file(credentials_path)
# structured_llm = ChatVertexAI(model="gemini-2.0-flash", temperature=0)
# generate_model_output('xin chào', 'hello')
from ragas.llms import LangchainLLMWrapper

os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model="gpt-4o-mini"))

from huggingface_hub import login
from ragas.evaluation import Dataset, evaluate

api_token = os.getenv("HUGGINGE_FACE_TOKEN")
login(api_token)

"""### **Experiment 6 - Changing the reranker document**

Keeping all other parameters constant, we changed the **reranker ways**, such as using api, using words search, hybrid search to see if we can improve performance by providing more context.

This time, our RAG setup looks like:
- **Embeddings: `bge-small-en`**
- **Chunk size: `1024`**
- **Chunk overlap: `128`**
- **Number of docs: `3`**
- **Language model: `Gemini`**
"""

import pandas as pd
from dotenv import load_dotenv

load_dotenv()
from utils.reranker_utils import *

reranker = Reranker(method="jina")

chunk_size = 512
chunk_overlap = 128
embedding_model_name = "BAAI/bge-base-en"
num_docs = 7
chunk_type = "character"

COLLECTION_NAME = f"experiment_hybrid_{chunk_size}_{chunk_overlap}_{embedding_model_name.split('/')[1]}_{chunk_type}"
# COLLECTION_NAME = f"experimenthybrid"
exp_hybrid = QdrantRAG(client=client)
# exp_hybrid.add_documents_hybrid(
#     collection_name=COLLECTION_NAME,
#     documents=all_page_documents,
#     tables=all_table_documents,
#     chunk_type=chunk_type,
#     chunk_size=chunk_size,
#     chunk_overlap=chunk_overlap,
#     embedding_model_name=embedding_model_name,
# # )

# # Trong notebook của bạn
# experiment_jina = run_ragas_eval(
#     eval_df=eval_df,
#     collection_name=COLLECTION_NAME,
#     doc_retrieval_function=lambda collection, query, embedding_model, num_documents, reranker:
#         exp_hybrid.get_documents_hybrid(
#             collection_name=collection,
#             query=query,
#             embedding_model_name=embedding_model,
#             num_documents=num_documents,
#             reranker=reranker,
#             retrieval_k=num_documents * 3  # ✅ Retrieve 21 docs, rerank to get top 7
#         ),
#     num_docs=num_docs,
#     embedding_model_name=embedding_model_name,
#     path=f"{COLLECTION_NAME}_{num_docs}_jina_gemma.csv",
#     reranker_function=reranker
# )

# experiment_jina = pd.read_csv(
#     "notebooks/Rag_Evaluator/experiment_hybrid_512_128_bge-base-en_character_7_jina_gemma.csv"
# )


# reranker = Reranker(method="mixedbread")
# experiment_mixbread = run_ragas_eval(
#     eval_df=eval_df,
#     collection_name=COLLECTION_NAME,
#     doc_retrieval_function=lambda collection,
#     query,
#     embedding_model,
#     num_documents,
#     reranker: exp_hybrid.get_documents_hybrid(
#         collection_name=collection,
#         query=query,
#         embedding_model_name=embedding_model,
#         num_documents=num_documents,
#         reranker=reranker,
#         retrieval_k=num_documents * 3,
#     ),
#     num_docs=num_docs,
#     embedding_model_name=embedding_model_name,
#     path=f"{COLLECTION_NAME}_{num_docs}_mixedbread_gemma.csv",
#     reranker_function=reranker,
# )

# reranker = Reranker(method="cohere")

# experiment_cohere = run_ragas_eval(
#     eval_df=eval_df,
#     collection_name=COLLECTION_NAME,
#     doc_retrieval_function=lambda collection,
#     query,
#     embedding_model,
#     num_documents,
#     reranker: exp_hybrid.get_documents_hybrid(
#         collection_name=collection,
#         query=query,
#         embedding_model_name=embedding_model,
#         num_documents=num_documents,
#         reranker=reranker,
#         retrieval_k=num_documents * 3,
#     ),
#     num_docs=num_docs,
#     embedding_model_name=embedding_model_name,
#     path=f"{COLLECTION_NAME}_{num_docs}_cohere_gemma.csv",
#     reranker_function=reranker,
# )

# experiment_names = [
#     "Exp 1: jina, gte-base, Gemini",
#     "Exp 2: mixbread, gte-base, Gemini",
#     "Exp 3: cohere, gte-base, Gemini",
# ]
# metrics_to_plot = ["context_precision", "context_recall", "hit_rate", "mrr"]

# # plot_experiment_comparison([experiment_jina, experiment_mixbread, experiment_cohere], experiment_names, metrics_to_plot)

"""##### **Using Open source Reranker**"""

import torch

if torch.cuda.is_available():
    print("CUDA is available. Using GPU for computations.")

chunk_size = 512
chunk_overlap = 128
embedding_model_name = "BAAI/bge-base-en"
num_docs = 7
chunk_type = "character"

COLLECTION_NAME = f"experiment_hybrid_{chunk_size}_{chunk_overlap}_{embedding_model_name.split('/')[1]}_{chunk_type}"

reranker = Reranker(method="bce")
experiment_bce = run_ragas_eval(
    eval_df=eval_df,
    collection_name=COLLECTION_NAME,
    doc_retrieval_function=lambda collection,
    query,
    embedding_model,
    num_documents,
    reranker: exp_hybrid.get_documents_hybrid(
        collection_name=collection,
        query=query,
        embedding_model_name=embedding_model,
        num_documents=num_documents,
        reranker=reranker,
        retrieval_k=num_documents * 3,
    ),
    num_docs=num_docs,
    embedding_model_name=embedding_model_name,
    path=f"{COLLECTION_NAME}_{num_docs}_bce_gemma.csv",
    reranker_function=reranker,
)

reranker = Reranker(method="colbert")

experiment_colbert = run_ragas_eval(
    eval_df=eval_df,
    collection_name=COLLECTION_NAME,
    doc_retrieval_function=lambda collection,
    query,
    embedding_model,
    num_documents,
    reranker: exp_hybrid.get_documents_hybrid(
        collection_name=collection,
        query=query,
        embedding_model_name=embedding_model,
        num_documents=num_documents,
        reranker=reranker,
        retrieval_k=num_documents * 3,
    ),
    num_docs=num_docs,
    embedding_model_name=embedding_model_name,
    path=f"{COLLECTION_NAME}_{num_docs}_colbert_gemma.csv",
    reranker_function=reranker,
)

reranker = Reranker(method="flashrank")

experiment_flashrank = run_ragas_eval(
    eval_df=eval_df,
    collection_name=COLLECTION_NAME,
    doc_retrieval_function=lambda collection,
    query,
    embedding_model,
    num_documents,
    reranker: exp_hybrid.get_documents_hybrid(
        collection_name=collection,
        query=query,
        embedding_model_name=embedding_model,
        num_documents=num_documents,
        reranker=reranker,
        retrieval_k=num_documents * 3,
    ),
    num_docs=num_docs,
    embedding_model_name=embedding_model_name,
    path=f"{COLLECTION_NAME}_{num_docs}_flashrank_gemma.csv",
    reranker_function=reranker,
)

reranker = Reranker(method="st-crossencoder")

experiment_crossencoder = run_ragas_eval(
    eval_df=eval_df,
    collection_name=COLLECTION_NAME,
    doc_retrieval_function=lambda collection,
    query,
    embedding_model,
    num_documents,
    reranker: exp_hybrid.get_documents_hybrid(
        collection_name=collection,
        query=query,
        embedding_model_name=embedding_model,
        num_documents=num_documents,
        reranker=reranker,
        retrieval_k=num_documents * 3,
    ),
    num_docs=num_docs,
    embedding_model_name=embedding_model_name,
    path=f"{COLLECTION_NAME}_{num_docs}_crossencoder_gemma.csv",
    reranker_function=reranker,
)

experiment_names = [
    "Exp 1: bce, gte-base, Gemini",
    "Exp 2: colbert, gte-base, Gemini",
    "Exp 3: flashrank, gte-base, Gemini",
    "Exp 4: st-cross-encoder, gte-base, Gemini",
]
metrics_to_plot = ["context_precision", "context_recall", "hit_rate", "mrr"]

# plot_experiment_comparison([experiment_bce, experiment_colbert, experiment_flashrank, experiment_crossencoder], experiment_names, metrics_to_plot)

"""##### **Using Hybrid Search Reranker**"""
