{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "eval_data = json.load(open(\"C:\\\\Users\\\\PC\\\\CODE\\\\WDM-AI-TEMIS\\\\data-finetune\\\\final_data\\\\pymupdf_json.json\", \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6\n",
    "\n",
    "ground_truth = eval_data[idx][\"markdown_content\"]\n",
    "predict = eval_data[idx][\"pymupdf_md\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea District[hide]|\n",
      "|--|--|--|--|--|--|--|--|--|--|--|--|--|--|\n",
      "|Month| Jan| Feb| Mar| Apr| May| Jun| Jul| Aug| Sep| Oct| Nov| Dec| Year|\n",
      "|Mean dailymaximum °C (°F)|26(79)|27(81)|25(77)|23(73)|20(68)|17(63)|17(63)|20(68)|23(73)|27(81)|27(81)|28(82)|23(74)|\n",
      "|Mean dailyminimum °C (°F)|15(59)|15(59)|13(55)|6(43)|6(43)|3(37)|1(34)|0(32)|5(41)|12(54)|14(57)|16(61)|9(48)|\n",
      "|Average rainfallmm (inches)|76(3.0)|98(3.9)|55(2.2)|28(1.1)|50(2.0)|34(1.3)|0(0)|0(0)|0(0)|22(0.9)|103(4.1)|68(2.7)|534(21.0)|\n",
      "|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|Source 1: [5]|\n",
      "|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|Source 2: [6]|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Climate data for Berea District [hide]|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|\n",
      "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
      "|Month|Jan|Feb M|ar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|Year|\n",
      "|Mean daily<br>maximum °C (°F)|26<br>(79)|27<br>(81) (|25<br>77)|23<br>(73)|20<br>(68)|17<br>(63)|17<br>(63)|20<br>(68)|23<br>(73)|27<br>(81)|27<br>(81)|28<br>(82)|23<br>(74)|\n",
      "|Mean daily<br>minimum °C (°F)|15<br>(59)|15<br>(59) (|13<br>55)|6<br>(43)|6<br>(43)|3<br>(37)|1<br>(34)|0<br>(32)|5<br>(41)|12<br>(54)|14<br>(57)|16<br>(61)|9<br>(48)|\n",
      "|Average rainfall<br>mm (inches)|76<br>(3.0)|98<br>(3.9) (|55<br>2.2)|28<br>(1.1)|50<br>(2.0)|34<br>(1.3)|0<br>(0)|0<br>(0)|0<br>(0)|22<br>(0.9)|103<br>(4.1)|68<br>(2.7)|534<br>(21.0|\n",
      "|[5]<br>Source 1:|[5]<br>Source 1:|[5]<br>Source 1:|[5]<br>Source 1:|[5]<br>Source 1:|[5]<br>Source 1:|[5]<br>Source 1:|[5]<br>Source 1:|[5]<br>Source 1:|[5]<br>Source 1:|[5]<br>Source 1:|[5]<br>Source 1:|[5]<br>Source 1:|[5]<br>Source 1:|\n",
      "|[6]<br>Source 2:|[6]<br>Source 2:|[6]<br>Source 2:|[6]<br>Source 2:|[6]<br>Source 2:|[6]<br>Source 2:|[6]<br>Source 2:|[6]<br>Source 2:|[6]<br>Source 2:|[6]<br>Source 2:|[6]<br>Source 2:|[6]<br>Source 2:|[6]<br>Source 2:|[6]<br>Source 2:|\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: LLM initialized with JSON mode.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"Lỗi: OPENAI_API_KEY không được tìm thấy.\")\n",
    "else:\n",
    "    llm = ChatOpenAI(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        model_kwargs={\n",
    "            \"response_format\": {\"type\": \"json_object\"}\n",
    "        }\n",
    "    )\n",
    "    print(\"DEBUG: LLM initialized with JSON mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "def evaluate_table_extraction(ground_truth: str, predict: str, llm_model = None, debug: bool = False) -> any: # Thay đổi kiểu trả về nếu cần\n",
    "   parser = JsonOutputParser()\n",
    "\n",
    "   prompt_template_str = \"\"\"\n",
    "   Evaluate the accuracy of a table extraction tool by comparing two markdown tables.\n",
    "\n",
    "   Consider the following criteria:\n",
    "   - **Content**: Ensure matches and events in GROUND TRUTH are accurately represented in PREDICT.\n",
    "   - **Formatting**: Check formatting consistency, including spacing and alignment.\n",
    "   - **Data Integrity**: Verify accurate data representation of dates, teams, scores, match titles, and attendance.\n",
    "   - **Order**: Confirm event sequence consistency.\n",
    "\n",
    "   # Steps\n",
    "\n",
    "   1. **Content Comparison**: Match rows and confirm data accuracy (team names, scores, etc.).\n",
    "   2. **Formatting Review**: Check column alignment and use of whitespace.\n",
    "   3. **Data Integrity Check**: Identify missing data or inconsistencies.\n",
    "   4. **Order Verification**: Ensure consistent event sequences.\n",
    "   5. **Assessment**: Note discrepancies and provide a score (0-1).\n",
    "\n",
    "   # Output Format\n",
    "\n",
    "   You must respond with a JSON object containing only a score field with a float value between 0 and 1.\n",
    "   For example:\n",
    "   {{\"score\": 0.85}}\n",
    "\n",
    "   ### GROUND TRUTH\n",
    "   {ground_truth}\n",
    "\n",
    "   ### PREDICT\n",
    "   {predict}\n",
    "\n",
    "   Remember: Your entire response must be a valid JSON object with only a \"score\" field containing a number between 0 and 1.\n",
    "   \"\"\"\n",
    "   prompt = PromptTemplate.from_template(template=prompt_template_str)\n",
    "\n",
    "   try:\n",
    "      chain = prompt | llm_model | parser\n",
    "      if debug:\n",
    "         print(\"DEBUG: Chain created successfully.\")\n",
    "\n",
    "      invocation_payload = {\n",
    "         \"ground_truth\": ground_truth,\n",
    "         \"predict\": predict\n",
    "      }\n",
    "      if debug:\n",
    "         print(f\"DEBUG: Invoking chain with payload (first 100 chars of each): \\nGround truth: {ground_truth[:100]}...\\nPredict: {predict[:100]}...\")\n",
    "\n",
    "      # Để kiểm tra riêng lẻ LLM (bỏ qua parser tạm thời):\n",
    "      # formatted_prompt = prompt.invoke(invocation_payload)\n",
    "      # print(f\"DEBUG: Formatted prompt sent to LLM:\\n{formatted_prompt.to_string()}\") # Hoặc .text tùy phiên bản\n",
    "      # llm_output = llm_model.invoke(formatted_prompt)\n",
    "      # print(f\"DEBUG: Raw output from LLM:\\n{llm_output}\")\n",
    "      # res = parser.parse(llm_output) # Parse thủ công nếu muốn kiểm tra parser\n",
    "\n",
    "      res = chain.invoke(invocation_payload)\n",
    "      if debug:\n",
    "         print(f\"DEBUG: Chain invocation successful. Result (res): {res}\")\n",
    "         print(f\"DEBUG: Type of res: {type(res)}\")\n",
    "      return res\n",
    "\n",
    "   except Exception as e:\n",
    "      if debug:   \n",
    "         print(f\"ERROR in evaluate_table_extraction: {e}\")\n",
    "         import traceback\n",
    "         traceback.print_exc() # In ra chi tiết lỗi và dòng gây lỗi\n",
    "      return \"\" # Hoặc None, hoặc một giá trị báo lỗi cụ thể"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Chain created successfully.\n",
      "DEBUG: Invoking chain with payload (first 100 chars of each): \n",
      "Ground truth: |Climate data for Berea District[hide]|Climate data for Berea District[hide]|Climate data for Berea ...\n",
      "Predict: |Climate data for Berea District [hide]|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Co...\n",
      "DEBUG: Chain invocation successful. Result (res): {'score': 0.75}\n",
      "DEBUG: Type of res: <class 'dict'>\n",
      "{'score': 0.75}\n"
     ]
    }
   ],
   "source": [
    "a = evaluate_table_extraction(ground_truth, predict, llm, debug=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [02:59<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6951533742331291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "total_score = 0\n",
    "\n",
    "\n",
    "for sample in tqdm(eval_data):\n",
    "    ground_truth = sample['markdown_content']\n",
    "    predict = sample['pymupdf_md']\n",
    "    score = evaluate_table_extraction(ground_truth, predict, llm)\n",
    "    total_score += score['score']\n",
    "    # print(score)\n",
    "print(total_score / len(eval_data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
