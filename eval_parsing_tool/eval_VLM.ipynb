{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "labels = json.load(open('C:\\\\Users\\\\PC\\\\CODE\\\\WDM-AI-TEMIS\\\\data-finetune\\\\final_data\\\\final.json', 'r', encoding='utf-8'))\n",
    "vlm_res = json.load(open('C:\\\\Users\\\\PC\\\\CODE\\\\WDM-AI-TEMIS\\\\data-finetune\\\\qwen_results_final_updated.json', 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: LLM initialized with JSON mode.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"Lỗi: OPENAI_API_KEY không được tìm thấy.\")\n",
    "else:\n",
    "    llm = ChatOpenAI(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        model_kwargs={\n",
    "            \"response_format\": {\"type\": \"json_object\"}\n",
    "        }\n",
    "    )\n",
    "    print(\"DEBUG: LLM initialized with JSON mode.\")\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate_table_extraction(ground_truth: str, predict: str, llm_model = None, debug: bool = False) -> any: # Thay đổi kiểu trả về nếu cần\n",
    "   parser = JsonOutputParser()\n",
    "\n",
    "   prompt_template_str = \"\"\"\n",
    "   Evaluate the accuracy of a table extraction tool by comparing two markdown tables.\n",
    "\n",
    "   Consider the following criteria:\n",
    "   - **Content**: Ensure matches and events in GROUND TRUTH are accurately represented in PREDICT.\n",
    "   - **Formatting**: Check formatting consistency, including spacing and alignment.\n",
    "   - **Data Integrity**: Verify accurate data representation of dates, teams, scores, match titles, and attendance.\n",
    "   - **Order**: Confirm event sequence consistency.\n",
    "\n",
    "   # Steps\n",
    "\n",
    "   1. **Content Comparison**: Match rows and confirm data accuracy (team names, scores, etc.).\n",
    "   2. **Formatting Review**: Check column alignment and use of whitespace.\n",
    "   3. **Data Integrity Check**: Identify missing data or inconsistencies.\n",
    "   4. **Order Verification**: Ensure consistent event sequences.\n",
    "   5. **Assessment**: Note discrepancies and provide a score (0-1).\n",
    "\n",
    "   # Output Format\n",
    "\n",
    "   You must respond with a JSON object containing only a score field with a float value between 0 and 1.\n",
    "   For example:\n",
    "   {{\"score\": 0.85}}\n",
    "\n",
    "   ### GROUND TRUTH\n",
    "   {ground_truth}\n",
    "\n",
    "   ### PREDICT\n",
    "   {predict}\n",
    "\n",
    "   Remember: Your entire response must be a valid JSON object with only a \"score\" field containing a number between 0 and 1.\n",
    "   \"\"\"\n",
    "   prompt = PromptTemplate.from_template(template=prompt_template_str)\n",
    "\n",
    "   try:\n",
    "      chain = prompt | llm_model | parser\n",
    "      if debug:\n",
    "         print(\"DEBUG: Chain created successfully.\")\n",
    "\n",
    "      invocation_payload = {\n",
    "         \"ground_truth\": ground_truth,\n",
    "         \"predict\": predict\n",
    "      }\n",
    "      if debug:\n",
    "         print(f\"DEBUG: Invoking chain with payload (first 100 chars of each): \\nGround truth: {ground_truth[:100]}...\\nPredict: {predict[:100]}...\")\n",
    "\n",
    "      # Để kiểm tra riêng lẻ LLM (bỏ qua parser tạm thời):\n",
    "      # formatted_prompt = prompt.invoke(invocation_payload)\n",
    "      # print(f\"DEBUG: Formatted prompt sent to LLM:\\n{formatted_prompt.to_string()}\") # Hoặc .text tùy phiên bản\n",
    "      # llm_output = llm_model.invoke(formatted_prompt)\n",
    "      # print(f\"DEBUG: Raw output from LLM:\\n{llm_output}\")\n",
    "      # res = parser.parse(llm_output) # Parse thủ công nếu muốn kiểm tra parser\n",
    "\n",
    "      res = chain.invoke(invocation_payload)\n",
    "      if debug:\n",
    "         print(f\"DEBUG: Chain invocation successful. Result (res): {res}\")\n",
    "         print(f\"DEBUG: Type of res: {type(res)}\")\n",
    "      return res\n",
    "\n",
    "   except Exception as e:\n",
    "      if debug:   \n",
    "         print(f\"ERROR in evaluate_table_extraction: {e}\")\n",
    "         import traceback\n",
    "         traceback.print_exc() # In ra chi tiết lỗi và dòng gây lỗi\n",
    "      return \"\" # Hoặc None, hoặc một giá trị báo lỗi cụ thể\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [03:01<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score: 0.7815950920245403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total_score = 0\n",
    "for sample in tqdm(labels):\n",
    "    image_path = sample['image_path']\n",
    "    # search for the same image in vlm_res\n",
    "    vlm_res_sample = None\n",
    "    for vlm_sample in vlm_res:\n",
    "        if vlm_sample['image_path'] == image_path:\n",
    "            vlm_res_sample = vlm_sample\n",
    "    if vlm_res_sample is None:\n",
    "        print(f\"Image {image_path} not found in vlm_res\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # danh gia\n",
    "    ground_truth = sample['markdown_content']\n",
    "    predict = vlm_res_sample['markdown_content']\n",
    "    \n",
    "    score = evaluate_table_extraction(ground_truth, predict, llm_model=llm, debug=False)\n",
    "    total_score += score['score']\n",
    "    # break\n",
    "    \n",
    "print(f\"Total score: {total_score/len(labels)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
